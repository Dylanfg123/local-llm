services:
  vllm:
    image: vllm/vllm-openai:latest
    command: >
      --model astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit
      --host 0.0.0.0
      --port 8000
      --dtype auto
      --api-key devkey123
      --max-model-len 4096
      --enforce-eager
    environment:
      - HF_TOKEN=${HF_TOKEN:-}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - "C:\\AI_Models:/root/.cache/huggingface"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    networks: [ llmnet ]

  rag-api:
    image: python:3.11-slim
    working_dir: /app
    command: sh -c "pip install --no-cache-dir fastapi uvicorn[standard] httpx && uvicorn app:app --host 0.0.0.0 --port 7000"
    volumes:
      - ./app.py:/app/app.py:ro
    environment:
      # TALK TO VLLM THROUGH THE UI PROXY OR DIRECTLY WITH /v1 (both OK)
      - VLLM_URL=http://vllm:8000
      - QDRANT_URL=http://qdrant:6333
      - TEI_URL=http://tei:8080
      - OPENAI_API_KEY=devkey123
    depends_on: [ vllm, qdrant, tei ]
    ports: ["7000:7000"]
    networks: [ llmnet ]

  ui:
    image: node:20-alpine
    working_dir: /app                   # ✅ critical (your old one wasn’t fixed)
    command: sh -c "npm i && node server.js"
    volumes:
      - ./ui:/app                       # ui/ contains server.js, package.json, public/index.html
    environment:
      - VLLM_URL=http://vllm:8000       # proxy target
      - RAG_URL=http://rag-api:7000
    ports: ["5173:5173"]
    depends_on: [ vllm, rag-api ]
    networks: [ llmnet ]

  qdrant:
    image: qdrant/qdrant:latest
    ports: ["6333:6333"]
    networks: [ llmnet ]

  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.5
    environment:
      - MODEL_ID=sentence-transformers/all-MiniLM-L6-v2
    ports: ["8080:80"]
    networks: [ llmnet ]

networks:
  llmnet:
    driver: bridge
